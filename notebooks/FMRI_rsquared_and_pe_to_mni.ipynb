{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data/ext/PhD/Studies/PrimingDecay'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import nibabel as nib\n",
    "import os        \n",
    "from config import root\n",
    "import re\n",
    "from scipy import stats\n",
    "from nipype.interfaces import fsl\n",
    "\n",
    "# import custom libraries\n",
    "from tools.glm import (add_regressors, array2dummies,\n",
    "                       n_back_array, n_back_series,\n",
    "                       glm, contrast)\n",
    "\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pearsonr(X, Y):\n",
    "    \n",
    "    # Assuming last dimension is time\n",
    "    X = X.T\n",
    "    Y = Y.T\n",
    "\n",
    "    # Pearson r\n",
    "    cov = np.sum((X - np.mean(X, axis=0)) * (Y - np.mean(Y, axis=0)), axis=0)\n",
    "    std = np.sum((X - np.mean(X, axis=0))**2, axis=0)**.5 * np.sum((Y - np.mean(Y, axis=0))**2, axis=0)**.5\n",
    "\n",
    "    del X\n",
    "    del Y\n",
    "    \n",
    "    return cov / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_yhat(pes, mats):\n",
    "    \n",
    "    yhat = np.zeros(pes[0].shape + (len(mats),))\n",
    "    for i, pe in enumerate(pes):\n",
    "        yhat += np.tile(np.reshape(pe, pes[0].shape + (1,)), len(mats)) * mats[mats.columns[i]].values\n",
    "    \n",
    "    del pes\n",
    "    del mats\n",
    "    \n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "subdir = \"_sub_id_sub_{sub_id}\"\n",
    "featmodel = \"pop_decay_featmodel_ses1/mapflow\"\n",
    "mat_runfname = \"_pop_decay_featmodel_ses1{run}/run{run}.mat\"\n",
    "\n",
    "all_paths = pd.DataFrame()\n",
    "\n",
    "for sub in range(1, 7):    \n",
    "    featmodel_subdir = os.path.join(\n",
    "        root, \"data\", \"working_dir\", \n",
    "        \"glm/pop_decay_firstlevel\", \n",
    "        subdir.format(sub_id=\"%03d\" % sub),\n",
    "        featmodel)\n",
    "\n",
    "    # Mat files\n",
    "    mat_paths = []\n",
    "    for i, run in enumerate(os.listdir(featmodel_subdir)):\n",
    "        mat_path = os.path.join(featmodel_subdir, mat_runfname.format(run=i))\n",
    "        mat_paths.append(mat_path)\n",
    "        if not os.path.exists(mat_path):\n",
    "            print(\"WARNING MAT DOES NOT EXIST\")\n",
    "    \n",
    "    # PE files\n",
    "    pe_subdir = os.path.join(\n",
    "        root, \"data\", \"output\", \n",
    "        \"pop_decay_filmgls\",\n",
    "        \"sub_%03d\" % sub\n",
    "    )\n",
    "    \n",
    "    pe_paths = []\n",
    "    for i, run in enumerate(os.listdir(pe_subdir)):\n",
    "        pe_path = os.path.join(pe_subdir, \"_pop_decay_filmgls{run}\".format(run=i))\n",
    "        pe_paths.append(pe_path)\n",
    "        if not os.path.exists(pe_path):\n",
    "            print(\"WARNING PE DOES NOT EXIST\")\n",
    "        \n",
    "    # Sort timeseries files\n",
    "    ts_path = os.path.join(root, \"data\", \"output\", \"highpass\")\n",
    "    sessions = sorted(re.findall(\"^ses_00[0-8]sub_%03d$\" % sub, \"\\n\".join(os.listdir(ts_path)), re.M))\n",
    "    ts_paths = []\n",
    "    ses_scn = []\n",
    "    for i, ses in enumerate(sessions):\n",
    "        runs = os.listdir(os.path.join(ts_path, ses))\n",
    "        for j, run in enumerate(runs):\n",
    "            ts_fname = os.path.join(\n",
    "                ts_path, ses, run, \"run_%03d_st_mcf_warp_dtype_bet_intnorm_smooth_hpf.nii.gz\" % j\n",
    "            )\n",
    "            ses_scn.append((i, j))\n",
    "            ts_paths.append(ts_fname)\n",
    "            if not os.path.exists(ts_fname):\n",
    "                print(\"WARNING TS DOES NOT EXIST\")\n",
    "    \n",
    "    ses, scn = zip(*ses_scn)\n",
    "    all_paths = all_paths.append(pd.DataFrame({\n",
    "        \"sub\" : sub,\n",
    "        \"ses\" : ses,\n",
    "        \"scn\" : scn,\n",
    "        \"run\" : range(len(mat_paths)),\n",
    "        \"mat\" : mat_paths,\n",
    "        \"pe\": pe_paths,\n",
    "        \"ts\": ts_paths,\n",
    "    }), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get TS data\n",
      "Load regs\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-caf437cdf025>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m                 \u001b[0;34m\"pe%d.nii.gz\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             ))\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mpes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Create baseline\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/neuro/lib/python3.6/site-packages/nibabel/dataobj_images.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, caching)\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_cache\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_cache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcaching\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fill'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/neuro/lib/python3.6/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masanyarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m     \"\"\"\n\u001b[0;32m--> 544\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/neuro/lib/python3.6/site-packages/nibabel/arrayproxy.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;31m# Read array and scale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_unscaled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mapply_read_scaling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slope\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/neuro/lib/python3.6/site-packages/nibabel/arrayproxy.py\u001b[0m in \u001b[0;36mget_unscaled\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    283\u001b[0m                                        \u001b[0moffset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m                                        \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m                                        mmap=self._mmap)\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mraw_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/neuro/lib/python3.6/site-packages/nibabel/volumeutils.py\u001b[0m in \u001b[0;36marray_from_file\u001b[0;34m(shape, in_dtype, infile, offset, order, mmap)\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'readinto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mdata_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mn_read\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0mneeds_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/neuro/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mimport\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEBADF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read() on write-only GzipFile object\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/neuro/lib/python3.6/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/neuro/lib/python3.6/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m             \u001b[0muncompress\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munconsumed_tail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for run in all_paths.index:\n",
    "    pe_path = all_paths.loc[run, \"pe\"]\n",
    "    ts_path = all_paths.loc[run, \"ts\"]\n",
    "    mat_path = all_paths.loc[run, \"mat\"]\n",
    "    r2_path = os.path.join(root, \"data\", \"output\", \"postprocessing\", \"partialrsquared\")\n",
    "    r2_path_sub = os.path.join(r2_path, \"%03d\" % all_paths.loc[run, \"sub\"])\n",
    "        \n",
    "    if not os.path.exists(r2_path):\n",
    "        os.mkdir(r2_path)\n",
    "    if not os.path.exists(r2_path_sub):\n",
    "        os.mkdir(r2_path_sub)\n",
    "        \n",
    "    mat_file = pd.read_csv(\n",
    "        mat_path,\n",
    "        skiprows=5, \n",
    "        header=None, \n",
    "        sep=\"\\t\", \n",
    "        usecols=list(range(16))[0::2]\n",
    "    )\n",
    "    \n",
    "    print(\"Get TS data\")\n",
    "    ts = nib.load(ts_path)\n",
    "    ts_data = ts.get_data()\n",
    "    \n",
    "    # Remove baseline\n",
    "    regs = mat_file.columns[:4].tolist()\n",
    "    mats = mat_file[regs]\n",
    "    pes = []\n",
    "    print(\"Load regs\")\n",
    "    for reg in regs:\n",
    "        pe = nib.load(\n",
    "            os.path.join(\n",
    "                pe_path,\n",
    "                \"pe%d.nii.gz\" % (int(reg) + 1)\n",
    "            ))\n",
    "        pes.append(pe.get_data())\n",
    "        \n",
    "    print(\"Create baseline\")\n",
    "    baseline = create_yhat(pes, mats)\n",
    "    res = ts_data - baseline\n",
    "    del baseline\n",
    "    del ts_data\n",
    "    \n",
    "    # Look at regs of interest\n",
    "    print(\"Load regs\")\n",
    "    regs = mat_file.columns[4:].tolist()\n",
    "    for reg in regs:\n",
    "        print(\"- doing\", reg)\n",
    "        other = regs.copy()\n",
    "        other.remove(reg)\n",
    "        opes = []\n",
    "        for o in other:\n",
    "            pe = nib.load(\n",
    "                os.path.join(\n",
    "                    pe_path, \n",
    "                    \"pe%d.nii.gz\" % (int(o) + 1)\n",
    "                ))\n",
    "            opes.append(pe.get_data())\n",
    "\n",
    "        yhat_res = create_yhat(opes, mat_file[other])\n",
    "        yhat_reg = create_yhat(\n",
    "            [nib.load(\n",
    "                os.path.join(pe_path, \"pe%d.nii.gz\" % (int(reg) +1))\n",
    "            ).get_data()], \n",
    "            mat_file[[reg]])\n",
    "        \n",
    "        R = pearsonr(res - yhat_res, yhat_reg)\n",
    "        R2 = R**2\n",
    "        \n",
    "        imgR2 = nib.Nifti1Image(R2.T, pe.affine)\n",
    "        \n",
    "        r2_path_run = os.path.join(r2_path_sub, \"pe\"+str(reg)+\"_\"+\"run_%d.nii.gz\" % all_paths.loc[run, \"run\"])\n",
    "        nib.save(imgR2, r2_path_run)\n",
    "        \n",
    "    del yhat_res\n",
    "    del yhat_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "regs = [8, 10, 12, 14]\n",
    "postprocessing = os.path.join(root, \"data\", \"output\", \"postprocessing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # correct axes\n",
    "# for run in all_paths.index:\n",
    "#     pe_path = all_paths.loc[run, \"pe\"]\n",
    "#     r2_path = os.path.join(postprocessing, \"partialrsquared\", \"%03d\" % all_paths.loc[run, \"sub\"])\n",
    "#     r2_new_path = os.path.join(postprocessing, \"partialrsquared_corr\")\n",
    "    \n",
    "#     if not os.path.exists(r2_new_path):\n",
    "#         os.mkdir(r2_new_path)\n",
    "        \n",
    "#     r2_new_path_subdir = os.path.join(r2_new_path, \"%03d\" % all_paths.loc[run, \"sub\"])\n",
    "\n",
    "#     if not os.path.exists(r2_new_path_subdir):\n",
    "#         os.mkdir(r2_new_path_subdir)\n",
    "        \n",
    "#     for reg in regs:     \n",
    "#         fname = \"pe%d_run_%d.nii.gz\" \n",
    "#         r2_path_run = os.path.join(r2_path, fname % (reg, all_paths.loc[run, \"run\"]))\n",
    "#         r2_new_path_run = os.path.join(r2_new_path_subdir, fname % (reg+1, all_paths.loc[run, \"run\"]))\n",
    "#         r2 = nib.load(r2_path_run)\n",
    "#         r2_data = r2.get_data().T\n",
    "        \n",
    "#         pe_path_run = os.path.join(pe_path, \"pe%d.nii.gz\" % (int(reg) + 1))\n",
    "#         pe = nib.load(pe_path_run)\n",
    "        \n",
    "#         imgR2 = nib.Nifti1Image(r2_data, pe.affine)\n",
    "#         nib.save(imgR2, r2_new_path_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "standard = os.path.join(root, \"data\", \"in_analysis\", \"nii\", \"standard\")\n",
    "mni_brain_standard_path = os.path.join(standard, \"MNI152_T1_2mm_brain.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warp to mni\n",
    "for run in all_paths.index:\n",
    "    sub = all_paths.loc[run, \"sub\"]\n",
    "    pe_path = all_paths.loc[run, \"pe\"]\n",
    "    r2_path = os.path.join(postprocessing, \"partialrsquared_corr\", \"%03d\" % sub)\n",
    "    for reg in regs:\n",
    "        r2_path_run = os.path.join(r2_path, \"pe%d_run_%d.nii.gz\" % (reg+1, all_paths.loc[run, \"run\"]))\n",
    "        pe_path_run = os.path.join(pe_path, \"pe%d.nii.gz\" % (reg+1))\n",
    "\n",
    "        r2_path_run_warped = os.path.join(r2_path, \"warped_pe%d_run_%d.nii.gz\" % (reg, all_paths.loc[run, \"run\"]))\n",
    "        pe_path_run_warped = os.path.join(pe_path, \"warped_pe%d.nii.gz\" % (int(reg) + 1))\n",
    "\n",
    "        warp_r2 = fsl.ApplyWarp(\n",
    "            in_file=r2_path_run,\n",
    "            out_file=r2_path_run_warped,\n",
    "            ref_file=mni_brain_standard_path,\n",
    "            field_file=os.path.join(root, \"data\", \"output\", \"register_to_standard\", \"sub_%03d\" % sub, \"orig_field.nii.gz\"),\n",
    "            premat=os.path.join(root, \"data\", \"output\", \"register_to_standard\", \"sub_%03d\" % sub, \"inplane_brain_bbreg_sub_%03d.mat\" % sub),\n",
    "        )\n",
    "        \n",
    "        warp_pe = fsl.ApplyWarp(\n",
    "            in_file=pe_path_run,\n",
    "            out_file=pe_path_run_warped,\n",
    "            ref_file=mni_brain_standard_path,\n",
    "            field_file=os.path.join(root, \"data\", \"output\", \"register_to_standard\", \"sub_%03d\" % sub, \"orig_field.nii.gz\"),\n",
    "            premat=os.path.join(root, \"data\", \"output\", \"register_to_standard\", \"sub_%03d\" % sub, \"inplane_brain_bbreg_sub_%03d.mat\" % sub),\n",
    "        )\n",
    "        \n",
    "        print(sub, run)\n",
    "        warp_r2.run()\n",
    "        warp_pe.run()                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr1_rois_path = os.path.join(postprocessing, \"clr1_replication.nii.gz\")\n",
    "clr2_rois_path = os.path.join(postprocessing, \"clr2_replication.nii.gz\")\n",
    "loc1_rois_path = os.path.join(postprocessing, \"loc1_replication.nii.gz\")\n",
    "loc2_rois_path = os.path.join(postprocessing, \"loc2_replication.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr1_rois_path = os.path.join(postprocessing, \"clr1_replication_peak_rois.nii.gz\")\n",
    "clr2_rois_path = os.path.join(postprocessing, \"clr2_replication_peak_rois.nii.gz\")\n",
    "loc1_rois_path = os.path.join(postprocessing, \"loc1_replication_peak_rois.nii.gz\")\n",
    "loc2_rois_path = os.path.join(postprocessing, \"loc2_replication_peak_rois.nii.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "clr1_rois = nib.load(clr1_rois_path).get_data()\n",
    "clr2_rois = nib.load(clr2_rois_path).get_data()\n",
    "loc1_rois = nib.load(loc1_rois_path).get_data()\n",
    "loc2_rois = nib.load(loc2_rois_path).get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PE\n",
    "rois_df = pd.DataFrame()\n",
    "for run in all_paths.index:\n",
    "    sub = all_paths.loc[run, \"sub\"]\n",
    "    pe_path = all_paths.loc[run, \"pe\"]\n",
    "    \n",
    "    # loc1\n",
    "    pe9 = nib.load(os.path.join(pe_path, \"warped_pe9.nii.gz\")).get_data()\n",
    "    pe9_vals = []\n",
    "    for i in range(1, int(loc1_rois.max())+1):\n",
    "        mask = loc1_rois == i\n",
    "        val = pe9[mask].mean()\n",
    "        pe9_vals.append(val)\n",
    "        \n",
    "    # loc2\n",
    "    pe11 = nib.load(os.path.join(pe_path, \"warped_pe11.nii.gz\")).get_data()\n",
    "    pe11_vals = []\n",
    "    for i in range(1, int(loc2_rois.max())+1):\n",
    "        mask = loc2_rois == i\n",
    "        val = pe11[mask].mean()\n",
    "        pe11_vals.append(val)\n",
    "        \n",
    "    # clr1\n",
    "    pe13 = nib.load(os.path.join(pe_path, \"warped_pe13.nii.gz\")).get_data()\n",
    "    pe13_vals = []\n",
    "    for i in range(1, int(clr1_rois.max())+1):\n",
    "        mask = clr1_rois == i\n",
    "        val = pe13[mask].mean()\n",
    "        pe13_vals.append(val)\n",
    "\n",
    "    # clr2\n",
    "    pe15 = nib.load(os.path.join(pe_path, \"warped_pe15.nii.gz\")).get_data()\n",
    "    pe15_vals = []\n",
    "    for i in range(1, int(clr2_rois.max())+1):\n",
    "        mask = clr2_rois == i\n",
    "        val = pe15[mask].mean()\n",
    "        pe15_vals.append(val)\n",
    "        \n",
    "    cols = [\"pe_loc1_roi%d\" % i for i in range(len(pe9_vals))]\n",
    "    cols += [\"pe_loc2_roi%d\" % i for i in range(len(pe11_vals))]\n",
    "    cols += [\"pe_clr1_roi%d\" % i for i in range(len(pe13_vals))]\n",
    "    cols += [\"pe_clr2_roi%d\" % i for i in range(len(pe15_vals))]\n",
    "    \n",
    "    df = pd.DataFrame([\n",
    "        pe9_vals + pe11_vals + pe13_vals + pe15_vals\n",
    "    ], columns=cols)\n",
    "    \n",
    "    df = df.assign(**{\n",
    "        \"sub\": sub,\n",
    "        \"ses\": all_paths.loc[run, \"ses\"],\n",
    "        \"scn\": all_paths.loc[run, \"scn\"]\n",
    "    })\n",
    "    \n",
    "    rois_df = rois_df.append(df)\n",
    "\n",
    "rois_df.to_csv(\"../data/pe_per_roi_per_contrast.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R2\n",
    "r2_rois_df = pd.DataFrame()\n",
    "for run in all_paths.index:\n",
    "    sub = all_paths.loc[run, \"sub\"]\n",
    "    r2_path = os.path.join(postprocessing, \"partialrsquared_corr\", \"%03d\" % sub)\n",
    "    \n",
    "    # loc1\n",
    "    r2pe9 = nib.load(os.path.join(r2_path, \"warped_pe8_run_%d.nii.gz\" % (all_paths.loc[run, \"run\"]))).get_data()\n",
    "    r2pe9_vals = []\n",
    "    for i in range(1, int(loc1_rois.max())+1):\n",
    "        mask = loc1_rois == i\n",
    "        val = r2pe9[mask].mean()\n",
    "        r2pe9_vals.append(val)\n",
    "        \n",
    "    # loc2\n",
    "    r2pe11 = nib.load(os.path.join(r2_path, \"warped_pe10_run_%d.nii.gz\" % (all_paths.loc[run, \"run\"]))).get_data()\n",
    "    r2pe11_vals = []\n",
    "    for i in range(1, int(loc2_rois.max())+1):\n",
    "        mask = loc2_rois == i\n",
    "        val = r2pe11[mask].mean()\n",
    "        r2pe11_vals.append(val)\n",
    "        \n",
    "    # clr1\n",
    "    r2pe13 = nib.load(os.path.join(r2_path, \"warped_pe12_run_%d.nii.gz\" % (all_paths.loc[run, \"run\"]))).get_data()\n",
    "    r2pe13_vals = []\n",
    "    for i in range(1, int(clr1_rois.max())+1):\n",
    "        mask = clr1_rois == i\n",
    "        val = r2pe13[mask].mean()\n",
    "        r2pe13_vals.append(val)\n",
    "        \n",
    "    # clr2\n",
    "    r2pe15 = nib.load(os.path.join(r2_path, \"warped_pe14_run_%d.nii.gz\" % (all_paths.loc[run, \"run\"]))).get_data()\n",
    "    r2pe15_vals = []\n",
    "    for i in range(1, int(clr2_rois.max())+1):\n",
    "        mask = clr2_rois == i\n",
    "        val = r2pe15[mask].mean()\n",
    "        r2pe15_vals.append(val)\n",
    "        \n",
    "    cols = [\"r2_loc1_roi%d\" % i for i in range(len(r2pe9_vals))]\n",
    "    cols += [\"r2_loc2_roi%d\" % i for i in range(len(r2pe11_vals))]\n",
    "    cols += [\"r2_clr1_roi%d\" % i for i in range(len(r2pe13_vals))]\n",
    "    cols += [\"r2_clr2_roi%d\" % i for i in range(len(r2pe15_vals))]\n",
    "    \n",
    "    df = pd.DataFrame([\n",
    "        r2pe9_vals + r2pe11_vals + r2pe13_vals + r2pe15_vals\n",
    "    ], columns=cols)\n",
    "    \n",
    "    df = df.assign(**{\n",
    "        \"sub\": sub,\n",
    "        \"ses\": all_paths.loc[run, \"ses\"],\n",
    "        \"scn\": all_paths.loc[run, \"scn\"]\n",
    "    })\n",
    "    \n",
    "    r2_rois_df = r2_rois_df.append(df)\n",
    "    \n",
    "r2_rois_df.to_csv(\"../data/r2_per_roi_per_contrast.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/regs_and_rts.csv\")\n",
    "\n",
    "pes = pd.DataFrame()\n",
    "\n",
    "for (sub, ses, scn), group in df.groupby([\"sub\", \"real_ses\", \"scn\"], as_index=False):\n",
    "    X = group[[\"loc1\", \"loc2\", \"loc3\", \"loc4\", \"lag1_loc\", \"lag2_loc\", \"lag1_clr\", \"lag2_clr\"]].astype(float)\n",
    "    Y = group.norm_rt\n",
    "    b = glm(Y, X.T)\n",
    "    \n",
    "    b[:4] * group[[\"loc1\", \"loc2\", \"loc3\", \"loc4\"]]\n",
    "    \n",
    "    baseline = (b[:4] * group[[\"loc1\", \"loc2\", \"loc3\", \"loc4\"]]).sum(axis=1)\n",
    "    res = Y - baseline\n",
    "    \n",
    "    regs = X.columns.tolist()[4:]\n",
    "    \n",
    "    rs = []\n",
    "    for i, reg in enumerate(regs):\n",
    "        other = regs.copy()\n",
    "        other.remove(reg)\n",
    "        \n",
    "        # Make use of b[i] corresponding to regs[i]\n",
    "        b_res = list(b[4:])\n",
    "        b_reg = b_res.pop(i)\n",
    "        \n",
    "        yhat_res = (b_res * group[other]).sum(axis=1)\n",
    "        yhat_reg = (b_reg * group[reg])\n",
    "\n",
    "        r, p = stats.pearsonr(res - yhat_res, yhat_reg)\n",
    "        rs.append(r**2)\n",
    "    \n",
    "    result = pd.DataFrame([b], columns=[\"loc1\", \"loc2\", \"loc3\", \"loc4\", \"rt_lag1_loc_pe\", \"rt_lag2_loc_pe\", \"rt_lag1_clr_pe\", \"rt_lag2_clr_pe\"])\n",
    "    result = result.join(pd.DataFrame([rs], columns=[reg+\"_r\" for reg in regs]))\n",
    "    result = result.assign(**{\n",
    "        \"sub\": sub,\n",
    "        \"ses\": ses,\n",
    "        \"scn\": scn,        \n",
    "    })\n",
    "    \n",
    "    pes = pes.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined = pes.set_index([\"sub\",\"ses\",\"scn\"]).join(r2_rois_df.set_index([\"sub\", \"ses\", \"scn\"])).join(rois_df.set_index([\"sub\", \"ses\", \"scn\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.to_csv(\"../data/joined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loc1                0.937396\n",
       "loc2                0.937274\n",
       "loc3                0.941655\n",
       "loc4                0.935159\n",
       "rt_lag1_loc_pe     -0.046079\n",
       "rt_lag2_loc_pe     -0.049054\n",
       "rt_lag1_clr_pe     -0.058310\n",
       "rt_lag2_clr_pe     -0.055059\n",
       "lag1_loc_r          0.021702\n",
       "lag2_loc_r          0.019975\n",
       "lag1_clr_r          0.035194\n",
       "lag2_clr_r          0.019228\n",
       "r2_loc1_roi0        0.010887\n",
       "r2_loc1_roi1        0.011273\n",
       "r2_loc2_roi0        0.011252\n",
       "r2_loc2_roi1        0.014217\n",
       "r2_loc2_roi2        0.013467\n",
       "r2_loc2_roi3        0.012427\n",
       "r2_clr1_roi0        0.012338\n",
       "r2_clr1_roi1        0.013476\n",
       "r2_clr1_roi2        0.011427\n",
       "r2_clr1_roi3        0.010749\n",
       "r2_clr1_roi4        0.014055\n",
       "r2_clr2_roi0        0.013533\n",
       "r2_clr2_roi1        0.016528\n",
       "r2_clr2_roi2        0.016731\n",
       "pe_loc1_roi0      -54.150868\n",
       "pe_loc1_roi1      -42.771125\n",
       "pe_loc2_roi0      -34.400681\n",
       "pe_loc2_roi1      -35.881559\n",
       "pe_loc2_roi2     -102.448685\n",
       "pe_loc2_roi3      -97.505835\n",
       "pe_clr1_roi0      -60.862518\n",
       "pe_clr1_roi1     -119.241110\n",
       "pe_clr1_roi2      -53.174565\n",
       "pe_clr1_roi3      -53.140790\n",
       "pe_clr1_roi4      -99.331554\n",
       "pe_clr2_roi0      -55.853485\n",
       "pe_clr2_roi1     -131.591430\n",
       "pe_clr2_roi2      -89.758763\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27150127140244673\n",
      "-0.2136651025025398\n",
      "0.10850914764393357\n",
      "0.5055561723310467\n",
      "0.2867419381680505\n",
      "0.4250077004464374\n"
     ]
    }
   ],
   "source": [
    "for idx, group in joined.groupby(\"sub\")[[\"rt_lag2_loc_pe\", \"pe_loc2_roi0\", \"pe_loc2_roi1\", \"pe_loc2_roi2\", \"pe_loc2_roi3\"]]:\n",
    "    \n",
    "    r, p = stats.pearsonr(group.rt_lag2_loc_pe, group.pe_loc2_roi3)\n",
    "    \n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20053638217121014\n",
      "0.45372245269402156\n",
      "0.3107356924415548\n",
      "0.3055887121858501\n",
      "0.543223577349776\n",
      "0.6400815581522875\n"
     ]
    }
   ],
   "source": [
    "for idx, group in joined.groupby(\"sub\")[[\"rt_lag2_clr_pe\", \"pe_clr2_roi0\", \"pe_clr2_roi1\", \"pe_clr2_roi2\"]]:\n",
    "    \n",
    "    r, p = stats.pearsonr(group.rt_lag2_clr_pe, group.pe_clr2_roi0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro",
   "language": "python",
   "name": "neuro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
